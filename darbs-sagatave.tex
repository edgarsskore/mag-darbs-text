\documentclass[12pt,paper=a4]{report}
\usepackage{fontspec}
\usepackage{xunicode}
\usepackage{xltxtra}
\usepackage{polyglossia}
\setdefaultlanguage{latvian}
\usepackage{fixlatvian}
\usepackage{caption}
\setotherlanguages{english,russian,french}
\setmainfont[Mapping=tex-text]{Times New Roman}%{LMRoman10}
% Fonts krievu valodai, kurā ir arī krievu valodas burti
\newfontfamily\russianfont{Times New Roman}
% Šos fontus tālāk izmantos chapter virsrakstos un url'os (lai būtu kirilicas burti)
\newfontfamily\sffamily{Verdana}
\captionsetup{justification=centering}
\usepackage{setspace}

% lai varam normāli rakstīt apakšvītras
\usepackage{underscore}
% Lai varam iekļaut attēlus
\usepackage{graphicx}
% Kurā vietā tiks meklēti attēli - relatīvais ceļs attiecībā pret dokumentu
\graphicspath{{./PNG/}{./images/internet/}{./images/self-generated/}}
% Ar šiem PDF'ā būs saliktas saites un tām va uzlikt krāsu
\usepackage{hyperref}
\hypersetup{ colorlinks, citecolor=black, filecolor=black,linkcolor=black,urlcolor=black }

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{lipsum} %Lai ģenerētu nejaušus tekstus...
\usepackage{listingsutf8}
\usepackage{xcolor}

%\usepackage{inconsolata}
\lstset{
    language=bash, %% Troque para PHP, C, Java, etc... bash é o padrão
    basicstyle=\ttfamily\small,
    numberstyle=\footnotesize,
    numbers=left,
    backgroundcolor=\color{gray!10},
    frame=single,
    tabsize=2,
    rulecolor=\color{black!30},
    title=\lstname,
    escapeinside={\%*}{*)},
    breaklines=true,
    breakatwhitespace=true,
    framextopmargin=2pt,
    framexbottommargin=2pt,
    extendedchars=false,
    inputencoding=utf8
}


%% Mainīt chapteru izskatu - centrēts un definētais sffamily fonts (skatīt augstāk)
\usepackage{titlesec}
\titleformat{\chapter}{\huge\centering\sffamily}{\thechapter}{1pc}{}

%% Pārdēvējam ``Literatūra`` par ``Izmantotās literatūras un avotu saraksts''.
\addto\captionslatvian{
\renewcommand\bibname{Izmantotās literatūras un avotu saraksts}
}

%% Atraitņrindiņas un bāreņrindiņas ( widow orphan) vadība
\clubpenalty10000
\widowpenalty10000

%% Visādas atkāpes - 1" (2.54 cm) atkāpe jau ir pēc noklusējuma, šeit tikai korekcijas
%\setlength{\parskip}{1line}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\textheight}{22.7cm}
\setlength{\textwidth}{15cm}
\setlength{\oddsidemargin}{0.5in}
\setlength{\evensidemargin}{0.5in}
%\setlength{\parindent}{0.25in}
%\setlength{\parskip}{0.25in}

%% uzliekam atkāpes arī nodaļu 1. rindkopas 1. rindai
\usepackage{indentfirst}

%Pārnesumiem - ļauj tiasīt lielākas starpas
\hyphenpenalty=5000

%% Nodaļu un apakšnodaļu numerācija tagad to veic fixlatvian package
%\def\thechapter      {\arabic{chapter}.}
%\def\thesection      {\ifx\chapter\undefined{\arabic{section}.}\else  %{\thechapter\arabic{section}.}\fi}
%\def\thesubsection   {\thesection\arabic{subsection}.}
%\def\thesubsubsection{\thesubsection\arabic{subsubsection}.}
%\def\theparagraph    {\thesubsubsection\arabic{paragraph}.}
%\def\thesubparagraph {\theparagraph\arabic{subparagraph}.}

%% Pakotne lai saliktu automātisku figūru skaitīšanu
\usepackage{totcount}

\newcounter{nofappendices}
\setcounter{nofappendices}{0}
\regtotcounter{nofappendices}

\newtotcounter{fignum}
\def\oldfigure{} \let\oldfigure=\figure
\def\figure{\stepcounter{fignum}\oldfigure}

%defineejam atsauču skaitītāju
\newtotcounter{citnum}
\def\oldbibitem{} \let\oldbibitem=\bibitem
\def\bibitem{\stepcounter{citnum}\oldbibitem}

%% Attēlu numerācija
%\renewcommand{\thefigure}{\arabic{chapter}.\arabic{figure}.}

\input{src/titullapas-dati} %šeit pārdefinējam savus mainīgos (atstāju iepriekšējās rindas, lai varētu redzēt pārdefinēšanu)

%% pievienota anotācijas noformēšana
%\usepackage{etoolbox}% http://ctan.org/pkg/etoolbox
%\makeatletter
%\patchcmd{\@makechapterhead}{\vspace*{50\p@}}{}{}{}% Removes space above \chapter head
%\setafterchapterskip{1sp}
%defineejam anotaacijas lapas, lai vareetu vienkaarshaak taas izmantot
\def\abstract{
  %\section*{\begin{center} \abstractname \end{center}} % start chapter

\vspace*{-4\baselineskip}
	\chapter*{\begin{center} \abstractname \end{center} } % start chapter
	\vspace*{-2.5\baselineskip}
  \addcontentsline{toc}{chapter}{\abstractname} % table of contents line
  \markboth{\MakeUppercase{\abstractname}}{} % header mark
}
\def\endabstract{}%\clearpage

%% Beidzot sākam rakstīt dokumentu
\begin{document}

%% Vislabāk nodaļas rakstīt kā atseviškus failus, kurus iekļauj ar input (.tex paplašinājums pats tiek pielikts klāt)
% \input{src/mag-titullapa} %% visu titullapu ērtāk ir turēt datnē mag-titullapa.tex, bet te mēs tomēr visu rakstīsim vienā vietā:

%%%% Titullapas sākums
\input{src/titullapa} %pievienota titullapa, kura izveidota atsevišķā failā
%%%% Titullapas beigas

%%%% Satura rādītājs
\tableofcontents

%%%% 1.5 līiniju atstarpe starp rindām
\onehalfspace

{
	\selectlanguage{latvian}
	\begin{abstract}
		
		\begin{tabular}{@{}r@{}l@{}}
			\parbox[c]{0.3\textwidth}{\textbf{The title:}}&
			\parbox[t]{0.65\textwidth}{\defDarbaNosaukumsEN} \\
			\parbox[c]{0.3\textwidth}{\textbf{Autors:}}&
			\parbox[t]{0.65\textwidth}{\defAutors} \\
			\parbox[c]{0.3\textwidth}{\textbf{Academic Advisor:}}&
			\parbox[t]{0.65\textwidth}{\defZinVaditajs} \\
			\parbox[c]{0.3\textwidth}{\textbf{The volume of the work:}}&
			\parbox[t]{0.65\textwidth}{\textcolor{black}{\pageref{LastPage}} pages, XX~tables,  \total{fignum}~images, XX~equations, \total{citnum}~literature sources, \total{nofappendices}~appendices} \\
			\parbox[c]{0.3\textwidth}{\textbf{Keywords:}}&
			\parbox[t]{0.65\textwidth}{ Life, Universe, Questions, Philosophy} \\
			&\\
		\end{tabular}
		%\total{nofimages} % ja nu gadiijumaa vajag custom counter
		
		In the first novel and radio series, a group of hyper-intelligent pan-dimensional beings demand to learn the \textbf{Answer to the Ultimate Question of Life} from the supercomputer, Deep Thought, specially built for this purpose. It takes Deep Thought 7½ million years to compute and check the answer, which turns out to be\textbf{ 42}.  Unfortunately, The Ultimate Question itself is unknown.\cite{wiki-en}
		
	\end{abstract}
}

%%%% 1.5 līiniju atstarpe starp rindām
\onehalfspace
{
\selectlanguage{english}
\begin{abstract}

\begin{tabular}{@{}r@{}l@{}}
\parbox[c]{0.3\textwidth}{\textbf{The title:}}&
\parbox[t]{0.65\textwidth}{\defDarbaNosaukumsEN} \\
\parbox[c]{0.3\textwidth}{\textbf{Author:}}&
\parbox[t]{0.65\textwidth}{\defAutors} \\
\parbox[c]{0.3\textwidth}{\textbf{Academic Advisor:}}&
\parbox[t]{0.65\textwidth}{\defZinVaditajs} \\
\parbox[c]{0.3\textwidth}{\textbf{The volume of the work:}}&
\parbox[t]{0.65\textwidth}{\textcolor{black}{\pageref{LastPage}} pages, XX~tables,  \total{fignum}~images, XX~equations, \total{citnum}~literature sources, \total{nofappendices}~appendices} \\
\parbox[c]{0.3\textwidth}{\textbf{Keywords:}}&
\parbox[t]{0.65\textwidth}{ Life, Universe, Questions, Philosophy} \\
&\\
\end{tabular}
%\total{nofimages} % ja nu gadiijumaa vajag custom counter

In the first novel and radio series, a group of hyper-intelligent pan-dimensional beings demand to learn the \textbf{Answer to the Ultimate Question of Life} from the supercomputer, Deep Thought, specially built for this purpose. It takes Deep Thought 7½ million years to compute and check the answer, which turns out to be\textbf{ 42}.  Unfortunately, The Ultimate Question itself is unknown.\cite{wiki-en}

\end{abstract}
}

%%%% Nodaļa bez numerācijas
\chapter*{Izmantotie saīsinājumi un termini}
\addcontentsline{toc}{chapter}{Izmantotie saīsinājumi un termini}

%%%  Sākas nodaļas
\input{src/ch-ievads} %% Ērtāk visu ir rakstīt atsevišķos failos - ievads.tex un tad tos iekļaut ar input komandu

\chapter{Mašīnmācīšanās}
Mašīnmācīšanās algoritmi ir kļuvuši par neatņemamu sastāvdaļu programmatūras izstrādātājiem un kompānijām, kas savas aplikācijas grib padarīt "gudras". Lai arī cik, mūsdienās, šis jēdziens ir kļuvis populārs, oficiāla mašīnmācīšanās definīcija nav noteikta. Visvienkāršākais mašīnmācīšanās pielietojums ir apstrādāt datus, mācīties no tiem un no iegūtajiem rezultātiem pieņemt lēmumus vai veikt minējumus reālās pasaules problēmu risināšanai. Tā vietā, lai manuāli veidotu programmatūras risinājumus, kas veic kādu uzdevumu, tiek apmācīti datori vai citas ierīces, izmantojot lielus datu apjomus. 
\section{Pamatjēdziens}
Pasaulē ir daudz un dažādi mašīnmācīšanās algoritmi un katru dienu tiek publicēti simtiem jaunu algoritmu. Tos var sagrupēt pēc apmācības veida (vadītā apmācība (\textit{supervised learning}), nevadītā apmācība (\textit{unsupervised learning}), pusvadītā apmācība (\textit{semi-supervised learning})) kā arī pēc formas vai funkcijas līdzībām (klasifikācija, regresija, lēmumu koki (\textit{decision trees}), klasterēšana (\textit{clustering}), dziļā mašīnmācīšanās (\textit{deep learning})). Neatkarīgi no apmācības veida vai pielietojuma, visas mašīnmācīšanās algoritmu kombinācijas sastāv no klasifikatoriem (atbalsta vektora mašīna, lēmuma koki, neironu tīkli), vērtēšanas funkcijām (varbūtības funkcijas, robežfunkcijas, izmaksu funkcija) un optimizācijas funkcijām (mantkārīgā meklēšana, nepārtrauktās optimizācijas metodes (\textit{continuous optimization})). Izmantojot šīs sastāvdaļas, mašīnmācīšanās algoritmu pamata mērķis ir būt spējīgam funkcionēt ne tikai ar apmācībā piedāvātajiem datiem, bet arī spēt darboties ar datiem, ar kuriem algoritms nav saskāries. 

Atkarībā no veicamā uzdevuma, ir dažādi veidi kā panākt, lai datori vai jebkura cita ierīce mācās, sākot ar visparastākajiem lēmumu kokiem, beidzot ar ģenētiskajiem algoritmiem un mākslīgajiem neironu tīkliem. Lēmumu koki atspoguļo reālās dzīves koka struktūru un tos izmanto gan klasifikācijas, gan regresijas problēmu risināšanā. Analizējot datus, lēmumu kokus var izmantot, lai vizuāli aprakstītu lēmumus un lēmumu pieņemšanu. Mašīnmācīšanās gadījumā, lēmumu kokus apraksta kā klasifikācijas kokus vai regresijas kokus (\textit{CART - Classification and Regression Trees}), atkarībā no veicamā uzdevuma.  Galvenā šo koku doma ir audzēt zarus, pieņemot lēmumus, kuras koka īpašības izvēlēties, zinot apstāšanās nosacījumu \cite{dectree}. Lai gan lēmumu koki nav vispopulārākais mašīnmācīšanās veids, taču tas ir pielietots klasifikācijas problēmu risināšanai dotajos pētījumos \cite{dectreepaper}\cite{pal2003assessment}.

Ģenētiskie algoritmi ir vēl viens algoritmu veids, kuru ir vērts izcelt. Tie ir algoritmi, kuri tiek izveidoti, balstoties uz notikumiem, kurus novēro dabīgajos evolūcijas procesos. Datorzinātnē ģenētiskie algoritmi ir optimizācijas algoritmi, kuri prot patstāvīgi apgūt jaunu informāciju, balstoties uz evolūcijas jēdzieniem kā dabīgā atlase un ģenētika. Ģenētisko algoritmu pamatideja ir simulēt Čārlza Dārvina piedāvāto teorēmu "izdzīvo stiprākais". Risinot problēmu, ģenētiskais algoritms saglabā tikai spēcīgākos indivīdus katrā paaudzē. Šie indivīdi sacenšas par resursiem un iespēju veidot nākamo paaudzi. Jaunās paaudzes tiek veidotas izvēloties vecākus no iepriekšējās paaudzes, veicot \textit{crossover} operāciju un mutāciju. Spēcīgākie indivīdi katrā paaudzē izveidos vairāk pēcnācēju nekā vājie indivīdi, tādējādi katra nākamā indivīdu paaudze kļūs labāka, galu galā iegūstot labāko rezultātu problēmas risināšanai. Beigu nosacījumu nosaka pirms algoritma izpildes, parasti, tiek noteikts paaudžu skaits vai kāds labāko indivīdu rezultātu slieksnis, kuru pēcnācēju paaudze pārsniedz. Ģenētiskie algoritmi gan nespēj risināt klasifikācijas problēmas, bet tos var lietot kā optimizācijas funkciju \cite{genopti} vai kā kārtošanas algoritmu \cite{deb2000fast}. Lai rēķinātu neironu tīklu svarus, var izmantot ģenētiskos algoritmus. \cite{genalg}
\begin{figure}[h]%
	\centering
	\includegraphics[height=6cm]{images/gen-algo-bilde.png} %
	\caption{Ģenētisko algoritmu modelis}%
	\label{fig:example}%
\end{figure}

Mākslīgie neironu tīkli ir viena no populārākajām mašīnmācīšanās izmantotajām metodēm. Tas ir neapstrādāts elektronisks modelis, kas balstīts uz smadzeņu bioloģisko neironu tīklu. Var teikt, ka šāda neironu tīkla modelis, līdzīgi kā smadzenes, mācās no pieredzes. Teorētiski, šādu smadzeņu modelēšana, paredz, ka šāds mašīnmācīšanās risinājums, neprasa dziļas tehniskas zināšanas bioloģijā vai datorzinātnē, bet ir jāspēj tīklu izveidot pareizi kopā saliekot vairākas slāņu kārtas. Šādas, bioloģijas iedvesmotas metodes uzskata par nākamo lielo soli datorzinātnes industrijā.\cite{staff} \par
Iedziļinoties mākslīgo neironu tīklu uzbūvē, neironu tīkls sastāv no daudz, savstarpēji savienotiem mezgliem, kur katrs no mezgliem veic kādu matemātisku operāciju. To, ko atgriež katrs mezgls, nosaka matemātiskā operācija, ko šis mezgls veic kā arī citi parametri, kas specifiski šim mezglam. Šie mezgli galu galā tiek grupēti un šos mezglu grupējumus sauc par slāņiem (no ang. val. - \textit{layer}). \par
Mākslīgie neironu tīkli satur sava veida "mācīšanās likumus", kas ir process, kad tiek mainīti mezglu savienojumu svari atkarībā no informācijas ievadē. 
Kad neironu tīkls ir apmācīts tik tālu, ka lietotājs ir apmierināts, tad tīklam var sākt piedāvāt datus, kuri tad iziet cauri visiem slāņiem, tādā veidā turpinot mācības uz sākumā izveidotā modeļa bāzes. Neironu tīklus ir arī iespējams pārtrenēt, kas nozīmē, ka tīkls atpazīst tikai vienu ienākošo datu tipu. Ja tā notiek, tad mācīšanās vairs nav iespējama. Mākslīgos neironu tīklus izmanto dažādu problēmu risināšanai, piemēram, rakstu zīmju atpazīšanai \cite{nnchars}, attēlu kompresēšanai \cite{dony1995neural} vai pat akciju tirgus analizēšanai \cite{kimoto1990stock}. Šī darba nolūkos autors neironu tīklus izmantos dziļajai apmācībai.
\begin{figure}[h]%
	\centering
	\includegraphics[height=5cm]{images/neironutikls.png} %
	\caption{Vienkāršs neironu tīkla modelis}%
	\label{fig:example}%
\end{figure}
\section{Dziļā mašīnmācīšanās}
Mašīnmācīšanās no dziļās mašīnmācīšanās atšķiras ar to, ka mašīnmācīšanās ir sarežģīti izmantot ļoti lielas datu kopas, taču ar dziļās apmācības metodēm tas ir iespējams. Dziļās mašīnmācīšanās metodes atgriež jebkādas vērtības sākot ar skaitliskām vērtībām, beidzot ar elementiem kā attēli, teksts vai skaņa, taču parastās mašīnmācīšanās metodes spēj atgriezt tikai skaitliskas vērtības kā, piemēram, klasifikācijas indeksu vai kādas funkcijas rezultātu. Mašīnmācīšanās izmanto dažādus automatizētus algoritmus, kas iemācās modeļa funkcijas un paredz nākotnes darbības no padotajiem datiem, kamēr dziļā apmācībā izmanto neironu tīklus, kas laiž datus caur daudz apstrādes slāņiem, lai izšķirtu datu īpašības. Lielākā atšķirība, pēc autora domām, starp mašīnmācīšanos un dziļo mašīnmācīšanos ir tajā, ka dziļās mašīnmācīšanās metodes automātiski datos atrod svarīgās īpašības, kamēr mašīnmācīšanās algoritmos šīs īpašības ir manuāli jānorāda. 
\begin{figure}[h]%
	\centering
	\includegraphics[height=7cm]{images/deeplearning.png} %
	\caption{Mašīnmācīšanās un dziļās mašīnmācīšanās salīdzinājums}%
	\label{fig:example}%
\end{figure}
Dziļā mašīnmācīšanās ļauj apmācīt matemātiskus modeļus, kas izveidoti no vairākiem datu apstrādes slāņiem, ar datiem, kas attēloti kā vairāku līmeņu abstrakcija (pētamā objekta galveno īpašību izdalīšana un mazsvarīgu aspektu ignorēšana). Šīs metodes ir uzlabojušas jaunākās tehnoloģijas balss atpazīšanā, objektu atpazīšanā attēlos, objektu detektēšanā. Dziļā mašīnmācīšanās, izmantojot atpakaļdatošanas algoritmus, sarežģītās datu kopu struktūrās meklē kā datoram vai jebkurai citai ierīcei būtu jāmaina iekšējie parametri starp tīklu slāņiem.

Īpašību apmācība ir metožu kopums, kas atļauj ierīcei padot neapstrādātus datus un automātiski iegūt īpašības, kas nepieciešamas, lai veiktu detektēšanu vai klasifikāciju. Dziļās mašīnmācīšanās metodes ir īpašību apmācības metodes ar vairākiem īpašību slāņiem, kurus iegūst apvienojot vienkāršus, taču nelineārus modeļus, kur katrs modelis pārveido īpašību no viena līmeņa uz augstāku, abstraktāku līmeni. Veicot pietiekami daudz šādus pārveidojumus, algoritmiem ir iespējams iemācīties ļoti sarežģītas darbības.\cite{deepnet} Darba ietvaros, objektu detektēšanai tiks izmantots konvolūciju neironu tīkls (\textit{CNN}), kas ir dziļās mašīnmācīšanās tips.

\section{Konvolūcijas neironu tīkli}
Konvolūcijas neironu tīkli (turpmāk \textit{CNN}) ir izveidoti, lai apstrādātu datus, kas ievadei padoti kā vairāki masīvi, piemēram, divdimensiju masīvi, kas satur pikseļu intensitātes vairākos krāsu kanālos (attēls). Vairākus datu veidus vienkāršojot, tos var izteikt kā masīvus: viendimensijas masīvs priekš signāliem vai skaitļu rindām, divdimensiju masīvi attēliem vai audio spektogrammām un trīsdimensiju masīvs video. Vislabāko rezultātu CNN tīklu veidi sasniedz risinot objektu detektēšanas \cite{li2015convolutional}\cite{matsugu2003subject}, segmentācijas \cite{long2015fully} vai klasifikācijas problēmas \cite{classif}\cite{krizhevsky2012imagenet}\cite{jia2014caffe}. Šī darba ietvaros CNN tiks izmantots objektu, precīzāk cilvēka galvu detektēšanai.

\subsection{Tīklu slāņi}
Līdzīgi parastajiem neironu tīkliem arī konvolūciju neironu tīkli ir izveidoti no vairākiem slāņiem. CNN ir sarežģītāka struktūra kā parastam neironu tīklam. Vienkārša konvolūciju neironu tīkla arhitektūra sastāv no vairākiem, secīgi novietotiem slāņiem, kurus var izdalīt pa tipiem: konvolūcijas slānis (no kurienes arī rodas tīkla nosaukums), nelinearitātes slānis jeb aktivizācijas slānis, apvienošanas slānis un pilnīgi savienotais slānis (līdzīgi kāds tiek izmantots parastajos neironu tīklos). 
\subsubsection{Konvolūcijas slānis}
Konvolūcijas slānis ir CNN galvenā sastāvdaļa. Kā jau no nosaukuma var noprast, konvolūcijas slānī tiek veikta konvolūcijas operācija. Tiek izvēlēts filtrs (kernelis), un šis filtrs tiek pārvietots pāri masīvam (kas var būt gan attēls, gan audio, gan video) un katrā pozīcijā veic konvolūcijas operāciju. No konvolūcijas operācijas iegūtās vērtības tiek saskaitītas un rezultātā iegūts viens skaitlis. Kad konvolūcijas operācija tiek veikta visam masīvam, tiek iegūts masīvs, ko sauc par īpašību karti (no angļu val. \textit{feature map}) un jo vairāk filtrus izmanto, jo dziļāks kļūst attēls. Izmantojot 32x32x3 ievades masīvu un 5x5x3 filtru (filtram jābūt tikpat dziļam, cik dziļš ir ievades masīvs, lai būtu iespējams veikt matricu reizinājumu) tiks iegūta 28x28x1 īpašību karte un jo vairāk filtri tiks pielietoti, jo dziļāka būs šī īpašību karte un vairāk īpašības būs iespējams atrast masīvā. 
\begin{figure}[h]%
	\centering
	\includegraphics[height=4cm]{images/ActivationMap.png} %
	\caption{Konvolūcijas operācija pirmajā solī pielietojot 5x5 izmēra filtru}%
	\label{fig:example}%
\end{figure}
\\
~
\\
~
\\
~
\\

Šī īpašību karte satur informāciju par to kur atrodas minētās īpašības un cik labi šīs īpašības iedarbojas ar filtru, tādējādi norādot cik ļoti katrā masīva punktā atrodas ar filtru raksturotais elements. Pirms konvolūcijas operācijas veikšanas, ir nepieciešams izvēlēties trīs lielumus, kas ietekmēs īpašību karti:

\begin{itemize}
	\item \textbf{Dziļums} - atbilst filtru skaitam, kas izvēlēts konvolūcijas operācijas veikšanai. Ja ar masīvu tiks veikta konvolūcijas operācija ar trīs dažādiem filtriem, tad tiks atgrieztas trīs dažādas īpašību kartes, kuras būs apvienotas vienā trīs dimensiju masīvā ar dziļumu trīs.
	\item \textbf{Solis} - norāda par cik pikseļiem ievades masīvā pārvietosies filtra masīvs. Piemēram, ja konvolūcijas operāciju veic attēlam un solis ir divi, tad filtri neiet cauri katram pikselim, bet katram otrajam sākot no attēla augšējā kreisā stūra. Jo lielāks solis, jo mazākas būs īpašību kartes, kas ir noderīgi, ja tiek izmantoti lieli vai arī daudz filtru. Tādējādi, ir iespējams ietaupīt skaitļošanas resursus. 
	\item \textbf{Nulles-apdare} (no angļu val. \textit{zero-padding}) - tiek izmantota, ievades matricai pieliekot nulles vērtības ap robežām, lai šiem robežu elementiem būtu pilnībā iespējams pielietot filtrus. Nulles-apdares labās īpašības var izmantot arī lai mainītu īpašību kartes izmērus. Izmantojot nulles-apdari, šis process tiek dēvēts par plato konvolūciju (no angļu val. \textit{wide convolution}), bet nulles-apdares neizmantošanu sauc par šauro konvolūciju (no angļu val. \textit{narrow convolution}). 
	\begin{figure}[h]%
		\centering
		\includegraphics[height=2.5cm]{images/zero-padding.png} %
		\caption{Nulles-apdares piemērs \cite{zerpad}}%
		\label{fig:example}%
	\end{figure}
\end{itemize}
\subsubsection{Nelinearitātes slānis}
Nelinearitātes slānis konvolūciju neironu tīklos satur aktivizācijas funkciju, kas ņem no konvolūcijas operācijas atgriezto īpašību karti un izveido aktivizācijas karti. Aktivizācijas funkcija veic matricas elementu reizinājumu izmantojot saņemtos datus, kas nozīmē, ka tiek izvadīts tik pat liels masīvs, cik bija saņemts ievadē. Aktivizācijas funkciju tradicionāli implementē kā sigmoīdu vai hiperbolisku tangensa funkciju un tās galvenais mērķis ir novērst linearitāti. Nesenāki pētījumi gan norāda, ka konvolūcijas neironu tīklos rektificētas lineārās vienības (no angļu val. \textit{rectified linear units}) (ReLUs) strādā labāk kā tradicionālās aktivizācijas funkcijas \cite{nair2010rectified}. 
\begin{figure}[h]%
	\centering
	\includegraphics[height=4cm]{images/sigmoidhiperbol.png} %
	\caption{Sigmoīds un hiperboliskā tangensa funkcija ir populāri\\ izmantotas aktivizācijas funkcijas konvolūcijas neironu tīklos}%
	\label{fig:example}%
\end{figure}

Rektificētās lineārās vienības (ReLUs) ir speciāla implementācija, kas apvieno nelinearitātes un rektifikācijas slāņu (rektifikācijas slānis atgriež ievades datu elementu moduli) operācijas konvolūcijas neironu tīklos. Rektificēta lineāra vienība ir gabalveida (no angļu val. \textit{piecewise}) lineāra funkcija, kas definēta sekojoši:
\[ Y_i^{(l)} = max(0,Y_i^{(l-1)}) \]
\begin{figure}[h]%
	\centering
	\includegraphics[height=4cm]{images/relu.png} %
	\caption{Rektificēta lineāra vienība}%
	\label{fig:example}%
\end{figure}

Konvolūcijas neironu tīklos, ReLUs ir trīs nozīmīgas priekšrocības pār tradicionālajām loģistikas funkcijām (sigmoīds) vai hiperboliskajām tangensa aktivizācijas funkcijām:
\begin{itemize}
	\item ReLUs efektīvi izplata gradientu, kas samazina iespēju saskarties ar gradienta izzušanas problēmu, kas ir parasta problēma dziļajās neironu tīklu arhitektūrās \cite{hochreiter1998vanishing}.
	\item Īpašības karšu negatīvās vērtības, rektifikācijas lineārā vienība pārveido par nullēm, tādējādi tiekot galā ar anulēšanas problēmu (no angļu val. \textit{cancellation problem}) kā arī izvadē iegūtā aktivizācijas karte saturēs daudz izsētāku vērtību apjomu. Šāds vērtību izkaisījums nodrošina stabilitāti gadījumā, ja ievadē notiek nelielas izmaiņas, piemēram, troksnis\cite{glorot2011deep}. 
	\item Ņemot vērā skaitļošanas sarežģītību, ReLUs sastāv no ļoti vienkāršām operācijām, kas nozīmē, ka šīs operācijas smagi neietekmē CNN veiktspēju, kas padara to implementēšanu konvolūcijas neironu tīklos ļoti efektīvu.
\end{itemize}
Šo priekšrocību dēļ, lielākā daļa jaunāko konvolūcijas neironu tīklu arhitektūru, piemēram, \cite{krizhevsky2012imagenet}\cite{DBLP:journals/corr/HeZR015}\cite{simonyan2014very}, izmanto tieši rektificētās lineārās vienības kā aktivizācijas funkciju nelinearitātes slānī.
\subsubsection{Apvienošanas slānis}
Apvienošanas slāņa (no angļu val. \textit{pooling layer}) galvenā funkcija ir samazināt aktivizāciju karšu dimensiju skaitu. Lai samazinātu iespēju pārlieku apmācīt tīklus (no angļu val. \textit{overfitting}) un samazinātu nepieciešamos skaitļošanas resursus, visbiežāk, šis slānis tiek pielietots pēc vairāk citu slāņu operāciju veikšanas (piemēram, pēc vairākiem konvolūcijas un aktivizācijas slāņiem). Apvienošanas operācijas mērķis ir saglabāt jau atrastās īpašības mazākā attēlojumā. Šis mērķis tiek sasniegts atmetot mazsvarīgos datus, lai iegūtu labāku telpisko izšķirtspēju. 

Apvienošanas slānī tiek definēts filtrs, kurš katrā operācijas solī tiks reducēts līdz vienai vērtībai. Līdzīgi kā konvolūcijas slānī, tiek izvēlēts solis pēc cik pozīcijām atkal tiks pielietots apvienošanas filtrs. Kad filtrs ir izmantots visām iespējamajām masīva pozīcijām, izvadē tiek iegūta telpiski samazināta aktivizācijas karte.

Visbiežāk izmantotās redukcijas metodes ir maksimumu apvienošana vai vidējās vērtības apvienošana. Maksimuma apvienošanas filtri meklē vislielāko vērtību filtra reģionā un atmet pārējās vērtības. Vidējās vērtības apvienošanas filtri saglabā filtr reģiona vidējo vērtību. Maksimuma apvienošana demonstrē spēju ātrāk konverģēt salīdzinājumā ar vidējās vērtības apvienošanu un citām metodēm, tāpēc to izmanto visbiežāk \cite{scherer2010evaluation}. 
\begin{figure}[h]%
	\centering
	\includegraphics[height=3cm]{images/maxpool.png} %
	\caption{Vienkāršs maksimuma apvienošanas slāņa modelis \cite{maxpool}}%
	\label{fig:example}%
\end{figure}
\subsubsection{Pilnīgi savienotais slānis}
Pilnīgās savienošanas slānis kā ievades datus saņem konvolūcijas, aktivizācijas vai apvienošanas slāņu izvadi un atgriež \textit{N} dimensiju vektoru, kur \textit{N} ir neironu tīklam piedāvāto klašu skaits. Šis klašu jeb atšķirīgu detektējamo objektu skaits ir jādefinē pirms tīkla apmācības. Iepriekš minētais vektors saturēs informāciju par to cik ļoti katrs aktivizācijas kartes elements korelē ar definētajām klasēm. Piemēram, ja tīklam ievadē ir padots attēls ar cilvēku, aktivizācijas kartē būs augstas vērtības īpašībām, kas raksturo, ka objektam ir divas rokas un kājas. 

\subsection{Tīklu arhitektūras}
\subsubsection{title1}
\subsubsection{title2}
\subsubsection{title11}
\subsubsection{title22}
\subsection{Tīklu apmācība}

\chapter{Detektēšana un sekošana}
aprakstīt kapēc detektēt un trackot, kapēc nevar tikai detektēt
\section{Detektēšana}
\subsection{SSD}
\subsection{YOLO}
\subsection{Faster R-CNN}
\section{Sekošana}
\subsection{AdaBoost}
\subsection{Multiple Instance Learning}
\subsection{Kernelized Correlation Filters}
\subsection{SiamFC}
\chapter{Implementācija}
\section{Datu sagatavošana}
\section{Detektēšana}
\subsection{caffe}
\subsubsection{Tīklu apmācība}
\subsubsection{Detektēšana}
\paragraph{SSD}
\paragraph{Faster R-CNN}
\subsection{darknet}
\subsubsection{Tīklu apmācība}
\subsubsection{Detektēšana izmantojot YOLO}
\section{Sekošana}
\subsection{OpenCV sekošanas programmējamā saskarne (API)}
\subsection{SiamFC}
\input{src/secinajumi-un-priekslikumi} %% Ērtāk visu failā secinajumi-un-priesklikumi.tex

\bibliographystyle{unsrt}
\selectlanguage{latvian}
\bibliography{src/links,src/articles,src/books}	%to load the *.bib files ../articles,../books,
\addcontentsline{toc}{chapter}{Izmantotās literatūras un avotu saraksts}


\chapter*{Galvojums}
\addcontentsline{toc}{chapter}{Galvojums}
 Ar šo es, \defAutors, galvoju, ka maģistra darbs ir izpildīts patstāvīgi un bez citu palīdzības. No svešiem pirmavotiem ņemtie dati un definējumi ir uzrādīti darbā. Šis darbs tādā vai citādā veidā nav nekad iesniegts nevienai citai pārbaudījumu komisijai un nav nekur publicēts.

\vspace{1in}
\defGads.gada \rule{1cm}{0.2pt}.\rule{3cm}{0.2pt}
\label{LastPage}

%% Te vajadzētu pielikumus
\appendix
\chapter{Pirmais pielikums}

\addtocounter{nofappendices}{1}
\label{appendix:tituldati}
%\lstinputlisting[firstline=6, lastline=15]{../codes/calcHomogInfModel.m}
\lstinputlisting[firstline=18, lastline=25]{README.md}

\chapter{Otrais pielikums}

\addtocounter{nofappendices}{1}
\label{appendix:tituldati2}
\lstinputlisting{README.md}

%% Vēl jāpievieno atzīmes lapa
\pagebreak
%% Šai lapai nevajag numerāciju
\pagestyle{empty}
\begin{center}
 Maģistra darbs aizstāvēts Valsts pārbaudījumu komisijas sēdē\\
 \vspace{1em}
\end{center}
\defGads.gada \rule{1cm}{0.2pt} . \rule{3cm}{0.2pt}\\\\
un novērtēts ar atzīmi \rule{4cm}{0.2pt} \\\\\\
Protokols Nr. \rule{1cm}{0.2pt}\\\\\\
Valsts pārbaudījumu komisijas \\\\
priekšsēdētājs \rule{7cm}{0.2pt}.\\
\hspace*{5cm}\textit{\raisebox{1em}{paraksts}}


\end{document}
